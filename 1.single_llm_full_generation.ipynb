{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dialogue Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the ollama server first, as a background process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "get_ipython().system = os.system  # a hack to allow running background processes from Jupyter notebook\n",
    "\n",
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Qwen 2.5 (14b) as our base LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"qwen2.5:14b\"  # The llm we want to use (https://ollama.com/library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have the model download for ollama to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 2049f5674b1e: 100% ▕██████████████████▏ 9.0 GB                         \u001b[K\n",
      "pulling 66b9ea09bd5b: 100% ▕██████████████████▏   68 B                         \u001b[K\n",
      "pulling eb4402837c78: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 832dd9e00a68: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling db59b814cab7: 100% ▕██████████████████▏  488 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ollama pull qwen2.5:14b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our selected model is now part of Ollama's available local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output (Dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining the JSON objects that we will use to represent the generated dialogues. For now this object will have only three fields: `\"model\"`, `\"seed\"`, `\"scenario\"`, and `\"dialog\"` to store the name of the model and the seed used to generate the dialogue, as well as the scenario associated to the dialogue and the dialogue itself, respectively. More preciselly, the `\"dialog\"` field will contain the list of turns of the conversation in order, with the speaker name and the corresponding utterances. As shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dialogue = {\n",
    "    \"model\": \"qwen2.5:14b\",  # the model used to generate the dialogue\n",
    "    \"seed\": 123,  # the seed used to generated\n",
    "    \"scenario\": \"short hello and good bye conversation\",  # the scenario used to generated the dialogue\n",
    "    \"turns\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pydantic` to properly define our `Dialogue` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "class Turn(BaseModel):\n",
    "    speaker: str\n",
    "    text: str\n",
    "\n",
    "class Dialog(BaseModel):\n",
    "    model: str  # the model used to generate the dialogue\n",
    "    seed: int  # the seed used to generated\n",
    "    scenario: Optional[Union[dict, str]] = None  # the scenario used to generated the dialogue\n",
    "    turns: List[Turn]  # the list of turns of the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Python `pydantic` class to formally represent our dialogues is quite useful, we can convert any JSON dialogue to our `Dialog` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue = Dialog.model_validate(example_dialogue)\n",
    "my_dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the opposite, convert our `Dialog`s to a `dict` or a JSON as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'qwen2.5:14b',\n",
       " 'seed': 123,\n",
       " 'scenario': 'short hello and good bye conversation',\n",
       " 'turns': [{'speaker': 'Alice', 'text': 'Hey Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Hey Alice!'},\n",
       "  {'speaker': 'Alice', 'text': 'Bye Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Bye bye!'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue.model_dump()  # a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"qwen2.5:14b\",\n",
      "  \"seed\": 123,\n",
      "  \"scenario\": \"short hello and good bye conversation\",\n",
      "  \"turns\": [\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Hey Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Hey Alice!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Bye Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Bye bye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_dialogue_json = my_dialogue.model_dump_json(indent=2)  # a string containing the dialog as a JSON object\n",
    "print(my_dialogue_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, of course, create a new `Dialog` from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario=None, turns=[Turn(speaker='Alice', text='Hi :)'), Turn(speaker='Bob', text='Bye! :(')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dialog(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    seed=123,\n",
    "    turns=[\n",
    "        Turn(speaker=\"Alice\", text=\"Hi :)\"),\n",
    "        Turn(speaker=\"Bob\", text=\"Bye! :(\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativelly, we can use the built-in `Dialog` class from `sdialog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(formatVersion='0.0.5', model='qwen2.5:14b', seed=123, dialogId=None, complete=None, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')], events=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog import Dialog\n",
    "\n",
    "my_dialog = Dialog.model_validate(example_dialogue)\n",
    "my_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which besides providing the exact same functionalities, among other things, allow us to:\n",
    "\n",
    "- Pretty print the dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print it in a vanilla textual form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: Hey Bob!\n",
      "Bob: Hey Alice!\n",
      "Alice: Bye Bob!\n",
      "Bob: Bye bye!\n"
     ]
    }
   ],
   "source": [
    "print(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either as a JSON object\n",
    "my_dialog.to_file(\"output/my_dialogue.json\")\n",
    "\n",
    "# or a txt file\n",
    "my_dialog.to_file(\"output/my_dialogue.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(check created files [`output/my_dialogue.json`](output/my_dialogue.json) and [`output/my_dialogue.txt`](output/my_dialogue.txt))_\n",
    "\n",
    "- Load a dialogue from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mshort hello and good bye conversation\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.json\")\n",
    "my_dialog.print(scenario=True)  # `scenario=True` to also print the metadata stored in scenario field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.txt\")\n",
    "my_dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or simple things like quickly know how long a dialogue is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin working on synthetic `Dialog` (;)) generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous section we defined what a synthetic dialogue looks like, which contains not only the conversational turns but also useful metadata.\n",
    "\n",
    "However, we want the LLM to generate the dialogue per se, not the metadata of course.\n",
    "\n",
    "Therefore, let's define now how we want the actual output of the LLM to look like.\n",
    "\n",
    "The simples is to use the same format as our `Dialog` but without metada fields, for instance, we would like the LLM to simply generate a JSON like this one:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"dialog\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Or as we already did with `Dialog`, we can formally define a `LLMDialogOutput` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the LLM output simply as a \"dialog\" field containing the list of turns\n",
    "class LLMDialogOutput(BaseModel):\n",
    "  dialog: List[Turn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need the LLM to generate a dialogue as such JSON object. \n",
    "\n",
    "The first thing we could try is to simply instruct the LLM in its prompt to _\"output a JSON object with a `\"dialog\"` key containing a list of turns, where each turn contains two keys, `\"speaker\"` and `\"text\"`, to save the speaker name and the utterance, respectively\"_.\n",
    "\n",
    "However, describing JSON objects with words in an unambiguous way is not an easy task and there's no guarantee the LLM will actually follow the instruction 100% of the times.\n",
    "\n",
    "Instead, we can work smarter (not harder! ;)): We can [force the LLM to generate an structured output](https://blog.danielclayton.co.uk/posts/ollama-structured-outputs/) by using a formal grammar to conditionate decoding.\n",
    "\n",
    "But wait, with `ollama` is even easier, since you can simply pass the [JSON **schema**](https://json-schema.org/overview/what-is-jsonschema) (yes, you guessed it, a JSON describing a JSON :)) that formally described the expected format of the output and will automatically do the work for us.\n",
    "\n",
    "But, how do I get the JSON schema? Don't worry! We don't have to do it manually!\n",
    "if your output is defined as a `pydantic` model, as we already did with the `Dialog` and `LLMDialogOutput`, **we can use the built-in `.model_json_schema()` method to obtain its JSON schema**, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Turn': {'properties': {'speaker': {'title': 'Speaker',\n",
       "     'type': 'string'},\n",
       "    'text': {'title': 'Text', 'type': 'string'}},\n",
       "   'required': ['speaker', 'text'],\n",
       "   'title': 'Turn',\n",
       "   'type': 'object'}},\n",
       " 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'},\n",
       "   'title': 'Dialog',\n",
       "   'type': 'array'}},\n",
       " 'required': ['dialog'],\n",
       " 'title': 'LLMDialogOutput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get the json schema for our defined Output\n",
    "LLMDialogOutput.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we know everything we need to know, we can force the LLM to always produce the output in such format as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dialog\": [\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Did you know penguins can't fly but they make excellent underwater acrobats?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"That's true, Alice. And did you know that kangaroos keep their babies in a backpack-like pouch? It's like they invented baby carriers before we did!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Speaking of oddities, have you ever considered what life would be like if bananas tasted like bubblegum?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Now that's a bizarre thought. Imagine trying to peel one and getting stuck with sticky fingers everywhere!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME,\n",
    "                 format=LLMDialogOutput.model_json_schema())\n",
    "\n",
    "# NOTE: note that here we're NOT giving a single instruction about how the output should look like\n",
    "llm_output = llm.invoke([(\"human\", \"generate a short and weird random dialogue between Alice and Bob\")]).content\n",
    "\n",
    "# and still, the output is a perfect JSON, as we wanted it :)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use everything we have learned so far to define a our own `DialogueGenerator` class that we can instantiate using different LLMs and descriptions to generate our dialogues or, better, we can use `sdialog`'s built-in one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.generators import DialogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which takes the following arguments as input:\n",
    "- `model` model name to use (any model tag from [ollama hub](https://ollama.com/library)).\n",
    "- `dialogue_details` the details about the desired dialogue.\n",
    "- `output_format` the output format as a `pydantic` class or JSON scheme, as we did above (`LLMDialogOutput` by default).\n",
    "- `scenario` an optional metadata field that describes the scenario used to generated dialogue.\n",
    "\n",
    "For instance, let's create an instance of `DialogGenerator` to generate conversations between Bob and Alice about her birthday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator = DialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    dialogue_details=\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Star Wars themed party.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the build-in `.generate()` method to generate conversations for such instance:\n",
    "\n",
    "_(each time you run the code below, a different dialogue will be generated)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m1000034307\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mThe conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Star Wars themed party.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mGood morning, Alice! What are your plans for today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! I'm planning my birthday party. Can we make it a Star Wars theme? It would be so cool if all my friends could come dressed as their favorite characters!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds like an amazing idea, Alice! What kind of activities or games do you think would fit the theme?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, I was thinking we could have a lightsaber duel area and maybe set up some obstacle courses. We could also have Jedi training where everyone can learn basic moves and even use the Force with some magic tricks!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThose sound like great ideas! How about snacks? Do you want to serve food that reminds us of Star Wars?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYes, please! I would love it if we could have blue milk and other foods from the movies. Maybe some cookies shaped like droids or spaceships.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mI think we can definitely do that with a bit of creativity in the kitchen. What about decorations? Do you already have ideas for that?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYes! We could use stars, planets, and alien creatures to decorate. Also, having some posters from Star Wars would make it look awesome!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSounds perfect! And don't forget the birthday cake - it should definitely have a space theme too.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat's right! I want a big galaxy-shaped cake with stars and planets on top. We can even put little toy droids inside for everyone to find!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mI think we've got all the elements planned out now - games, food, decorations, and of course, your amazing birthday cake! It's going to be a fantastic party, Alice.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThank you so much, Dad! I'm really excited about it. You're the best!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mYou're welcome, sweetheart. Let's start planning the details and making arrangements right away. Have a wonderful day!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThanks again, Dad. See you later!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change the description, now the party has to be about Lord of the Rings, not Star Wars. To do this we can use the `.set()` method to set new details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator.set(\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m308408483\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mThe conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mGood morning, Alice! How are you today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad, I'm good thanks! Dad, guess what? My birthday is coming up soon!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat's right, it is. What kind of party do you want to have this year?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI've been thinking about it a lot and I really want to throw a Lord of the Rings themed party! Can we do that?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds like a fun theme, Alice. What kind of things were you thinking for decorations or activities?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking maybe we could have a ring toss game, and I want to make my own costumes for the characters in Lord of the Rings! And we can decorate with banners and streamers that have characters on them.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat all sounds great, Alice. Let's start planning out what you need for your party. What are some other ideas you had?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, I was thinking maybe we could make food and drinks that match the theme too! Like 'Hobbiton Herb Tea' or a 'Shire Cheese Platter'. And there could be a movie marathon of all Lord of the Rings movies.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThose are some fantastic ideas, Alice. I think your friends will really enjoy it. When would you like to have the party?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHow about on Saturday afternoon? That way we could start with snacks and games and then watch the movies in the evening.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds perfect. I'll help you plan everything, Alice. Let's make your birthday party an unforgettable Lord of the Rings adventure!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThank you so much, Dad! I can't wait to have my party now.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mI'm excited for it too, Alice. Happy planning and let's make sure your birthday is extra special this year.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the seed number above to re-generate the exact same dialogue each time as an argument of `generate()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m4216355045\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice, how are you doing today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! I'm great, thanks for asking. Actually, there's something really important that I want to talk about.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSure thing, what's up? Is everything okay?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYes, yes, everything is good. My birthday is coming up soon and I've been thinking of having a party but not just any party!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOh, really? What kind of party are you planning then?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, Dad, I want to have it as a Lord of the Rings themed birthday party! It's my favorite movie series and I would love if everyone could come dressed up as their favorite character.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mAh, that sounds fun! What do you think we need for this kind of party?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWe might need to get some decorations like the Shire or Mordor and games based on the story. Oh, maybe we could also have food like 'Frodo's Famous Fried Chicken' or 'Sam's Secret Stew'.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds really creative! How about invites? Do you want me to help design them?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat would be awesome, Dad. And maybe we could also put some riddles on the invitations like in the books?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSure thing, I can do that. Let's make sure your friends will all have a great time and feel comfortable.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThanks Dad! You're always so supportive of my ideas. I really appreciate it.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOf course, sweetheart. That's what dads are for. We'll make your birthday party unforgettable!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI can't wait to start planning this with you! Thank you so much.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mAnytime, my dear. Let's get started on the planning as soon as we can!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate(4216355045).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Dialogue Generation for STAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ Before we begin this section, make sure you have the STAR dataset downloaded in your system, inside the `datasets` folder:\n",
    "> ```bash\n",
    "> cd datasets\n",
    "> git clone git@github.com:RasaHQ/STAR.git\n",
    "> ```\n",
    "> Make sure you have a `datasets/STAR` folder the `dialogues` and `tasks` folders inside.\n",
    "\n",
    "The [STAR](https://arxiv.org/pdf/2010.11853) dataset contains 6652 human-generated dialogues as JSON objects where files are named as `NUMBER.json`.\n",
    "\n",
    "Humans had to follow a well-defined set of instruction to generate the dialogue role playing the system (wizard) and the client (user).\n",
    "\n",
    "For instance, clicking [here](datasets/STAR/dialogues/1.json) we can open the file [`1.json`](datasets/STAR/dialogues/1.json) containing the first dialogue. For now, let's focus only on the `\"Scenario\"` field.\n",
    "\n",
    "For instance, for the dialogue in `1.json` it is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Domains\": [  # List of domains\n",
    "        \"doctor\"\n",
    "    ],\n",
    "    \"Happy\": true,  # Wheather or not the dialogue follos a happy path\n",
    "    \"MultiTask\": false,  # Wheather or not this dialogue involves more than one task\n",
    "    \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
    "    \"WizardTask\": \"Inform the user of his/her doctor's orders.\",\n",
    "    \"WizardCapabilities\": [  # List of flowcharts describing the each task the Wizard is cable of doing\n",
    "        {\n",
    "        \"Domain\": \"doctor\",\n",
    "        \"SchemaImage\": \"doctor_followup.jpg\",\n",
    "        \"Task\": \"doctor_followup\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We can use the `STAR` module from `sdialog` to read scenarios object from any dialogue in STAR given it's id given a STAR conversation id as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['doctor'],\n",
       " 'Happy': True,\n",
       " 'MultiTask': False,\n",
       " 'UserTask': 'You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.',\n",
       " 'WizardCapabilities': [{'Domain': 'doctor',\n",
       "   'SchemaImage': 'doctor_followup.jpg',\n",
       "   'Task': 'doctor_followup'}],\n",
       " 'WizardTask': \"Inform the user of his/her doctor's orders.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog.datasets import STAR\n",
    "\n",
    "# Let's first indicate where the dataset is located\n",
    "STAR.set_path(\"datasets/STAR/\")\n",
    "\n",
    "# Let's load the scenario of the first dialog\n",
    "scenario = STAR.get_dialog_scenario(1)\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which corresponds to the following dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mHello, I'm really worried. I forgot what I'm supposed to do and forgot to write it down... What do I do?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mMy name is Alexis and my last doctor was Dr. Morgan, but now my doctor is Dr. Johnson and I forgot how to take my medicine.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYour instructions are: Take your medicine before you go to sleep. If you experience nausea, please contact your doctor immediately..\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mAre you sure I'm supposed to take it before bed? I don't go to sleep every day because my sleep schedule is totally off right now because of the Coronavirus.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYes. It must be before bed or it will not be effective.\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mOkay thank you. I will get back in touch if this doesn't help.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "original_dialogue = STAR.get_dialog(1)\n",
    "original_dialogue.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `scenario`, we can see that in this conversation, the user's behavior is defined by instructions given in natural language (`\"UserTask\"`), however, the system/wizard behavior is more rigidly defined as a graph describing the dialogue policy to followed (since system was expected to be more deterministic). These graphs are described as JSON objects storing the graph edges as key:value pairs (source:destination). We can find these graphs in the [`STAR/tasks`](datasets/STAR/tasks) folder.\n",
    "\n",
    "Ideally, we would like our `DialogGenerator` to generate dialogues for each different scenario. That is, given a `scenario` we would like generate multiple dialogues for it.\n",
    "\n",
    "To achieve this, we only need to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Fortunately, we can use the built-in `get_scenario_description()` method to do this, which takes an `scenario` as input and returns its natural language description containing all the details (including the system behavior described by the graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation is between a User and a AI assistant in the following domains: doctor.\n",
      "\n",
      "The User instructions are: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n",
      "The AI assistant instructions are: Inform the user of his/her doctor's orders.\n",
      "\n",
      "In addition, the AI assistant is instructed to follow specific flowcharts to address the tasks. Flowcharts are defined as graph described using DOT.\n",
      "The actual DOT for the current tasks are:\n",
      "\n",
      "The graph for the task 'doctor_followup' with domain 'doctor' is:\n",
      "```dot\n",
      "digraph doctor_followup  {\n",
      "    hello -> ask_name;\n",
      "    ask_name -> doctor_ask_doctor_name;\n",
      "    doctor_ask_doctor_name -> query;\n",
      "    query -> doctor_inform_doctors_instructions;\n",
      "    doctor_inform_doctors_instructions -> anything_else\n",
      "}\n",
      "```\n",
      "and one example responses for each node is provided in the following json:\n",
      "```json\n",
      "{\n",
      "  \"hello\": \"Hello, how can I help?\",\n",
      "  \"ask_name\": \"Could I get your name, please?\",\n",
      "  \"doctor_ask_doctor_name\": \"Who is your doctor?\",\n",
      "  \"doctor_inform_doctors_instructions\": \"Your instructions are: INSTRUCTIONS.\",\n",
      "  \"doctor_bye\": \"Thank you and goodbye.\",\n",
      "  \"anything_else\": \"Is there anything else that I can do for you?\"\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Finally, the following should be considered regarding the conversation:\n",
      "   1. The conversation follows the 'happy path', meaning the conversations goes according to what it is described in the flowcharts.\n",
      "   2. The user is calling to perform only the defined task (doctor_followup), nothing else.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(STAR.get_scenario_description(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the original graph in JSON describing the system's behavior have been converted to a [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) description which should be easier to interpret by the LLM, since DOT is a well-known format to describe graphs in plain text. \n",
    "\n",
    "Let's now put these two methods together and create a function that given a STAR dialogue ID will generate a natural language description of the scenario associated to it, simply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_description(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Then return it along its description in natural language\n",
    "    return scenario, STAR.get_scenario_description(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function now we have everything we need to generate synthethic dialogues for STAR that follows the same scenario as a given target real dialogue.\n",
    "\n",
    "For instance, let's say we want to generate dialogues following the same scenario as the first STAR dialogue, we can simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the scenario and description of the first dialogue\n",
    "scenario, description = get_dialog_scenario_description(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = DialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate multiple conversation that follows the same scenario as dialogue 1 of STAR dataset.\n",
    "> **Note**\n",
    "> Run the cell multiple times to get different conversations for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m4148127142\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35m{\n",
      "  \"Domains\": [\n",
      "    \"doctor\"\n",
      "  ],\n",
      "  \"Happy\": true,\n",
      "  \"MultiTask\": false,\n",
      "  \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
      "  \"WizardCapabilities\": [\n",
      "    {\n",
      "      \"Domain\": \"doctor\",\n",
      "      \"SchemaImage\": \"doctor_followup.jpg\",\n",
      "      \"Task\": \"doctor_followup\"\n",
      "    }\n",
      "  ],\n",
      "  \"WizardTask\": \"Inform the user of his/her doctor's orders.\"\n",
      "}\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mCould you help me find out what my doctor's instructions were for my medication dosage?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mOf course, could I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mMy name is Alexis.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mDr. Morgan.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mYour instructions are: Take the medication three times a day, 30 minutes before each meal.\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mThank you so much!\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the LLM is able to follow the scenario surprisegnly well, specially for the system part which is guided by a graph with pre-defined responses.\n",
    "\n",
    "Now update the generator to match a more challenging scenario, let's say that of dialogue 5100 that is multi-task and does not follow a happy path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['plane', 'weather'],\n",
       " 'Happy': False,\n",
       " 'MultiTask': True,\n",
       " 'UserTask': 'Come up with your own scenario!\\n\\nAbout you:\\n- Your name: Ben\\n\\n The AI Assistant can handle:\\n- Search for a flight (e.g. from Chicago to Pittsburgh)\\n- Book a flight (e.g. with id 193)\\n- Checking the weather forecast in different Cities (e.g. Chicago or Pittsburgh)',\n",
       " 'WizardCapabilities': [{'Domain': 'plane',\n",
       "   'SchemaImage': 'plane_search.jpg',\n",
       "   'Task': 'plane_search'},\n",
       "  {'Domain': 'plane', 'SchemaImage': 'plane_book.jpg', 'Task': 'plane_book'},\n",
       "  {'Domain': 'weather', 'SchemaImage': 'weather.jpg', 'Task': 'weather'}],\n",
       " 'WizardTask': 'Follow the flow charts and help the user.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario, description = get_dialog_scenario_description(5100)\n",
    "\n",
    "dialog_generator.set(description, scenario)\n",
    "\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate dialogues for it:\n",
    "\n",
    "_(run multi-times the call to generate different ones)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m1817119516\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mHi there! First, could you help me find a flight from Chicago to Pittsburgh for next Wednesday morning?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mSure, my name is Ben.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mWhere are you departing from?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mI'm looking to depart from Chicago O'Hare International Airport.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mWhere are you going to?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mI want to arrive in Pittsburgh International Airport.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mOn what day do you want the plane to arrive?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mCan I have a flight next Wednesday morning, around 9 AM or so?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mI can also filter the results for a specific airline, class, price or duration.\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mLet's see the cheapest options available please.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mRight, I found UA 193 flight with United Airlines for $250 that takes 1 hour and 47 minutes to get to Pittsburgh.\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mGreat! Can you book this flight for me now?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mSure, can I have your flight ID, please?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mThe flight ID is 193.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mThe flight is available. Should I reserve it for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mActually, hold on a second. There's been some last minute changes at work and now I'm not sure if I'll need to travel next week anymore.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate one synthetic dialog for each happy `\"doctor_followup\"` dialog in STAR and save it to disk for later use.\n",
    "\n",
    "Let's first get all happy dialogues for this task using `sdialog`'s built-in `STAR.get_dialogs()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517c927f111f4f44bada2277448ed4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dialogs:   0%|          | 0/6652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of happy \"doctor_followup\" dialogues in STAR: 105\n"
     ]
    }
   ],
   "source": [
    "original_dialogs = STAR.get_dialogs(task_name=\"doctor_followup\", happy=True, multitask=False)\n",
    "print('Total number of happy \"doctor_followup\" dialogues in STAR:', len(original_dialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dialogues and save them in the path pointed by the `PATH_OUTPUT` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f08ca08fe64020949276a6210bf920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dialog generation:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH_OUTPUT = \"output/STAR/full-generation\"\n",
    "\n",
    "path_txt = os.path.join(PATH_OUTPUT, \"txt\")\n",
    "path_json = os.path.join(PATH_OUTPUT, \"json\")\n",
    "os.makedirs(path_txt, exist_ok=True)\n",
    "os.makedirs(path_json, exist_ok=True)\n",
    "\n",
    "for dialog in tqdm(original_dialogs, desc=\"Dialog generation\"):\n",
    "    if os.path.exists(os.path.join(path_json, f\"{dialog.dialogId}.json\")):\n",
    "        continue\n",
    "\n",
    "    scenario, description = STAR.get_dialog_scenario_description(dialog.dialogId)\n",
    "    dialog_generator = DialogGenerator(\n",
    "        model=MODEL_NAME,\n",
    "        dialogue_details=description,\n",
    "        scenario=scenario\n",
    "    )\n",
    "    dialog = dialog_generator.generate(id=dialog.dialogId, seed=dialog.dialogId)\n",
    "\n",
    "    # Normalize speaker names in each turn (since their also generated by the LLM)\n",
    "    for turn in dialog.turns:\n",
    "        turn.speaker = \"System\" if \"AI\" in turn.speaker else \"User\"\n",
    "\n",
    "    dialog.to_file(os.path.join(path_txt, f\"{dialog.dialogId}.txt\"))\n",
    "    dialog.to_file(os.path.join(path_json, f\"{dialog.dialogId}.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the files were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mjson\u001b[0m/\n",
      "\u001b[01;34mtxt\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls output/STAR/full-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "1848.txt\n",
      "1886.txt\n",
      "1896.txt\n",
      "1899.txt\n",
      "1942.txt\n",
      "1963.txt\n",
      "1970.txt\n",
      "2103.txt\n",
      "2228.txt\n",
      "2273.txt\n",
      "2487.txt\n",
      "2526.txt\n",
      "2578.txt\n",
      "2579.txt\n",
      "2624.txt\n",
      "2699.txt\n",
      "2733.txt\n",
      "3007.txt\n",
      "3024.txt\n",
      "3050.txt\n",
      "3071.txt\n",
      "3073.txt\n",
      "3086.txt\n",
      "3088.txt\n",
      "3110.txt\n",
      "3116.txt\n",
      "3126.txt\n",
      "3136.txt\n",
      "3155.txt\n",
      "3198.txt\n",
      "3202.txt\n",
      "3210.txt\n",
      "3234.txt\n",
      "3254.txt\n",
      "3264.txt\n",
      "3269.txt\n",
      "3274.txt\n",
      "3298.txt\n",
      "3316.txt\n",
      "3323.txt\n",
      "3329.txt\n",
      "3330.txt\n",
      "3356.txt\n",
      "3371.txt\n",
      "3391.txt\n",
      "3403.txt\n",
      "3418.txt\n",
      "3422.txt\n",
      "3437.txt\n",
      "3446.txt\n",
      "3454.txt\n",
      "3469.txt\n",
      "3494.txt\n",
      "3516.txt\n",
      "3528.txt\n",
      "3550.txt\n",
      "3652.txt\n",
      "3675.txt\n",
      "3743.txt\n",
      "3769.txt\n",
      "4055.txt\n",
      "4058.txt\n",
      "4067.txt\n",
      "4076.txt\n",
      "4082.txt\n",
      "4093.txt\n",
      "4100.txt\n",
      "4111.txt\n",
      "4159.txt\n",
      "4173.txt\n",
      "4191.txt\n",
      "4205.txt\n",
      "4214.txt\n",
      "4224.txt\n",
      "4231.txt\n",
      "4233.txt\n",
      "4253.txt\n",
      "4255.txt\n",
      "4263.txt\n",
      "4341.txt\n",
      "4349.txt\n",
      "4358.txt\n",
      "4381.txt\n",
      "4395.txt\n",
      "4403.txt\n",
      "4416.txt\n",
      "4459.txt\n",
      "4468.txt\n",
      "4477.txt\n",
      "4502.txt\n",
      "4515.txt\n",
      "4524.txt\n",
      "4536.txt\n",
      "4570.txt\n",
      "4591.txt\n",
      "4623.txt\n",
      "4653.txt\n",
      "4737.txt\n",
      "4743.txt\n",
      "4752.txt\n",
      "4851.txt\n",
      "4870.txt\n",
      "4875.txt\n",
      "9.txt\n"
     ]
    }
   ],
   "source": [
    "%ls output/STAR/full-generation/txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

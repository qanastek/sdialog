{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dialogue Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the ollama server first, as a background process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "get_ipython().system = os.system  # a hack to allow running background processes from Jupyter notebook\n",
    "\n",
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Qwen 2.5 (14b) as our base LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"qwen2.5:14b\"  # The llm we want to use (https://ollama.com/library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have the model download for ollama to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 2049f5674b1e: 100% ▕██████████████████▏ 9.0 GB                         \u001b[K\n",
      "pulling 66b9ea09bd5b: 100% ▕██████████████████▏   68 B                         \u001b[K\n",
      "pulling eb4402837c78: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 832dd9e00a68: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling db59b814cab7: 100% ▕██████████████████▏  488 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ollama pull qwen2.5:14b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our selected model is now part of Ollama's available local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED               \n",
      "qwen2.5:14b        7cdf5a0187d5    9.0 GB    Less than a second ago    \n",
      "llama3.2:1b        baf6a787fdff    1.3 GB    11 days ago               \n",
      "gemma3:4b          a2af6cc3eb7f    3.3 GB    2 weeks ago               \n",
      "gemma3:27b         a418f5838eaf    17 GB     6 weeks ago               \n",
      "deepseek-r1:14b    ea35dfe18182    9.0 GB    7 weeks ago               \n",
      "deepseek-r1:32b    38056bbcbb2d    19 GB     7 weeks ago               \n",
      "gemma3:1b          8648f39daa8f    815 MB    7 weeks ago               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output (Dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining the JSON objects that we will use to represent the generated dialogues. For now this object will have only three fields: `\"model\"`, `\"seed\"`, `\"scenario\"`, and `\"dialog\"` to store the name of the model and the seed used to generate the dialogue, as well as the scenario associated to the dialogue and the dialogue itself, respectively. More preciselly, the `\"dialog\"` field will contain the list of turns of the conversation in order, with the speaker name and the corresponding utterances. As shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dialogue = {\n",
    "    \"model\": \"qwen2.5:14b\",  # the model used to generate the dialogue\n",
    "    \"seed\": 123,  # the seed used to generated\n",
    "    \"scenario\": \"short hello and good bye conversation\",  # the scenario used to generated the dialogue\n",
    "    \"turns\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pydantic` to properly define our `Dialogue` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "class Turn(BaseModel):\n",
    "    speaker: str\n",
    "    text: str\n",
    "\n",
    "class Dialog(BaseModel):\n",
    "    model: str  # the model used to generate the dialogue\n",
    "    seed: int  # the seed used to generated\n",
    "    scenario: Optional[Union[dict, str]] = None  # the scenario used to generated the dialogue\n",
    "    turns: List[Turn]  # the list of turns of the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Python `pydantic` class to formally represent our dialogues is quite useful, we can convert any JSON dialogue to our `Dialog` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue = Dialog.model_validate(example_dialogue)\n",
    "my_dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the opposite, convert our `Dialog`s to a `dict` or a JSON as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'qwen2.5:14b',\n",
       " 'seed': 123,\n",
       " 'scenario': 'short hello and good bye conversation',\n",
       " 'turns': [{'speaker': 'Alice', 'text': 'Hey Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Hey Alice!'},\n",
       "  {'speaker': 'Alice', 'text': 'Bye Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Bye bye!'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue.model_dump()  # a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"qwen2.5:14b\",\n",
      "  \"seed\": 123,\n",
      "  \"scenario\": \"short hello and good bye conversation\",\n",
      "  \"turns\": [\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Hey Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Hey Alice!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Bye Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Bye bye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_dialogue_json = my_dialogue.model_dump_json(indent=2)  # a string containing the dialog as a JSON object\n",
    "print(my_dialogue_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, of course, create a new `Dialog` from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario=None, turns=[Turn(speaker='Alice', text='Hi :)'), Turn(speaker='Bob', text='Bye! :(')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dialog(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    seed=123,\n",
    "    turns=[\n",
    "        Turn(speaker=\"Alice\", text=\"Hi :)\"),\n",
    "        Turn(speaker=\"Bob\", text=\"Bye! :(\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativelly, we can use the built-in `Dialog` class from `sdialog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(formatVersion='0.0.5', model='qwen2.5:14b', seed=123, dialogId=None, complete=None, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')], events=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog import Dialog\n",
    "\n",
    "my_dialog = Dialog.model_validate(example_dialogue)\n",
    "my_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which besides providing the exact same functionalities, among other things, allow us to:\n",
    "\n",
    "- Pretty print the dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print it in a vanilla textual form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: Hey Bob!\n",
      "Bob: Hey Alice!\n",
      "Alice: Bye Bob!\n",
      "Bob: Bye bye!\n"
     ]
    }
   ],
   "source": [
    "print(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either as a JSON object\n",
    "my_dialog.to_file(\"output/my_dialogue.json\")\n",
    "\n",
    "# or a txt file\n",
    "my_dialog.to_file(\"output/my_dialogue.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(check created files [`output/my_dialogue.json`](output/my_dialogue.json) and [`output/my_dialogue.txt`](output/my_dialogue.txt))_\n",
    "\n",
    "- Load a dialogue from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mshort hello and good bye conversation\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.json\")\n",
    "my_dialog.print(scenario=True)  # `scenario=True` to also print the metadata stored in scenario field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.txt\")\n",
    "my_dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or simple things like quickly know how long a dialogue is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin working on synthetic `Dialog` (;)) generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous section we defined what a synthetic dialogue looks like, which contains not only the conversational turns but also useful metadata.\n",
    "\n",
    "However, we want the LLM to generate the dialogue per se, not the metadata of course.\n",
    "\n",
    "Therefore, let's define now how we want the actual output of the LLM to look like.\n",
    "\n",
    "The simples is to use the same format as our `Dialog` but without metada fields, for instance, we would like the LLM to simply generate a JSON like this one:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"dialog\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Or as we already did with `Dialog`, we can formally define a `LLMDialogOutput` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the LLM output simply as a \"dialog\" field containing the list of turns\n",
    "class LLMDialogOutput(BaseModel):\n",
    "  dialog: List[Turn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need the LLM to generate a dialogue as such JSON object. \n",
    "\n",
    "The first thing we could try is to simply instruct the LLM in its prompt to _\"output a JSON object with a `\"dialog\"` key containing a list of turns, where each turn contains two keys, `\"speaker\"` and `\"text\"`, to save the speaker name and the utterance, respectively\"_.\n",
    "\n",
    "However, describing JSON objects with words in an unambiguous way is not an easy task and there's no guarantee the LLM will actually follow the instruction 100% of the times.\n",
    "\n",
    "Instead, we can work smarter (not harder! ;)): We can [force the LLM to generate an structured output](https://blog.danielclayton.co.uk/posts/ollama-structured-outputs/) by using a formal grammar to conditionate decoding.\n",
    "\n",
    "But wait, with `ollama` is even easier, since you can simply pass the [JSON **schema**](https://json-schema.org/overview/what-is-jsonschema) (yes, you guessed it, a JSON describing a JSON :)) that formally described the expected format of the output and will automatically do the work for us.\n",
    "\n",
    "But, how do I get the JSON schema? Don't worry! We don't have to do it manually!\n",
    "if your output is defined as a `pydantic` model, as we already did with the `Dialog` and `LLMDialogOutput`, **we can use the built-in `.model_json_schema()` method to obtain its JSON schema**, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Turn': {'properties': {'speaker': {'title': 'Speaker',\n",
       "     'type': 'string'},\n",
       "    'text': {'title': 'Text', 'type': 'string'}},\n",
       "   'required': ['speaker', 'text'],\n",
       "   'title': 'Turn',\n",
       "   'type': 'object'}},\n",
       " 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'},\n",
       "   'title': 'Dialog',\n",
       "   'type': 'array'}},\n",
       " 'required': ['dialog'],\n",
       " 'title': 'LLMDialogOutput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get the json schema for our defined Output\n",
    "LLMDialogOutput.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we know everything we need to know, we can force the LLM to always produce the output in such format as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"dialog\": [\n",
      "  { \"speaker\": \"Alice\", \"text\": \"Bob, did you know that pineapples can't actually grow on trees?\" },\n",
      "  { \"speaker\": \"Bob\", \"text\": \"Really? Then where do they come from—outer space?\" },\n",
      "  { \"speaker\": \"Alice\", \"text\": \"Sort of. Pineapples grow at the center of a plant, like a big flower, not on a tree.\" },\n",
      "  { \"speaker\": \"Bob\", \"text\": \"Wow, and I thought my belief in flying spaghetti monsters was strange!\" }\n",
      "] }\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME,\n",
    "                 format=LLMDialogOutput.model_json_schema())\n",
    "\n",
    "# NOTE: note that here we're NOT giving a single instruction about how the output should look like\n",
    "llm_output = llm.invoke([(\"human\", \"generate a short and weird random dialogue between Alice and Bob\")]).content\n",
    "\n",
    "# and still, the output is a perfect JSON, as we wanted it :)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use everything we have learned so far to define a our own `DialogueGenerator` class that we can instantiate using different LLMs and descriptions to generate our dialogues or, better, we can use `sdialog`'s built-in one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.generators import DialogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which takes the following arguments as input:\n",
    "- `model` model name to use (any model tag from [ollama hub](https://ollama.com/library)).\n",
    "- `dialogue_details` the details about the desired dialogue.\n",
    "- `output_format` the output format as a `pydantic` class or JSON scheme, as we did above (`LLMDialogOutput` by default).\n",
    "- `scenario` an optional metadata field that describes the scenario used to generated dialogue.\n",
    "\n",
    "For instance, let's create an instance of `DialogGenerator` to generate conversations between Bob and Alice about her birthday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator = DialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    dialogue_details=\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Star Wars themed party.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the build-in `.generate()` method to generate conversations for such instance:\n",
    "\n",
    "_(each time you run the code below, a different dialogue will be generated)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3164540885\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mThe conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Star Wars themed party.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Dad, can we talk about my birthday party?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOf course, sweetie! What's on your mind for your special day?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, I've been thinking... what if we have a Star Wars themed party? Can we do that?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mA Star Wars party? That sounds like fun! What kind of things were you thinking about for the party?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking, maybe decorations with lightsabers and the Death Star. And everyone could come dressed as their favorite character!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds really cool. Do you have a list of who you'd like to invite? And do you know what games or activities they might enjoy?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI've got a guest list and I thought we could play some games like 'Pin the Blaster on Darth Vader' or maybe even have a Jedi training obstacle course!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThose sound like great ideas! We'll need to make sure there are snacks and drinks too. Maybe some blue milk for everyone?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat would be awesome! And what about cake? Can we have a giant Yoda cake or something?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mA Yoda cake, now that sounds like the best idea ever! We'll need to start planning this soon. How about you make a list of everything you'd like and we can go over it together later this week?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mSounds good, Dad! Thanks so much for helping me plan my Star Wars party!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mNo problem at all. I'm excited to help make your birthday one you'll never forget.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mMe too! Bye, Dad. Love you!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mLove you too, sweetheart. Have a great day!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change the description, now the party has to be about Lord of the Rings, not Star Wars. To do this we can use the `.set()` method to set new details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator.set(\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2444532898\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mThe conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice, how are you doing today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! I'm great, thanks for asking. Actually, there's something important I wanted to talk to you about.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSure thing, what's on your mind?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, my birthday is coming up soon and I've been thinking... What if we throw a Lord of the Rings themed party for my birthday? Wouldn't that be awesome?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mA Lord of the Rings party? That does sound fun! What kind of things were you thinking about doing?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking we could set up different stations based on each of the books. There could be a Hobbiton-style snack table, and maybe activities like ring toss or quiz games related to the stories.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds great! What else do you need from me?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was hoping you could help with decorations. And I might ask some friends for costumes, but we'll still need a few extra outfits and props around the house.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOf course! Decorating sounds like it will be fun. Let's start planning what kind of things we can do together.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat would be amazing, thanks Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mNo problem at all. I'm excited to see this come together and help make your birthday memorable.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThanks so much! You're the best dad ever!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mYou're welcome, kiddo. Let's get started planning right away then.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the seed number above to re-generate the exact same dialogue each time as an argument of `generate()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m4216355045\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHey Alice, how are you today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! I'm great, thanks for asking. Actually, there's something really important that I want to talk to you about.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSure thing, what's on your mind?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, my birthday is coming up soon and I've been thinking of having a special party. There's this movie series called 'The Lord of the Rings' that I really love. Could we maybe have a theme party based on it?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds like an interesting idea! What kind of things do you have in mind for the party?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking we could invite all my friends, decorate with some cool props and maybe even dress up as our favorite characters. We could also watch one or two movies together.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat does sound like a lot of fun! Do you have any specific ideas about decorations or games?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking we could make some banners with scenes from the movie and maybe set up areas in my room that look like different parts of Middle-earth. As for games, there are lots of quizzes and challenges based on the stories.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThose sound like great ideas! How about food? Do you have any thoughts on that?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYeah, I think we could have some Hobbiton-themed snacks and drinks. Maybe some second breakfast or elevenses platters would be fun.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds delicious! Alright, it seems like a well thought out plan to me. Let's do it!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mReally? You'd help me with that?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOf course, I'd be happy to help make your birthday special and memorable.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThank you so much, Dad! This is going to be the best party ever!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mI'm sure it will be. Let's start planning right away then.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mGreat idea! Can we do that now?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mSure, let's sit down and brainstorm some more details. Happy planning!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate(4216355045).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-Playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our gould here will be to have to LLM to generate the dialogues by role-playing the different charecters.\n",
    "\n",
    "Each character will be fully defined by its persona, so, the same way started this tutorial by defining what a \"synthetic `Dialog`\" will actually be, we should now define our `Persona`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the `sdialog` contains a `BasePersona` that we can import to create our own custom persona classes, let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.personas import BasePersona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define our concrete `Persona` class now by specifying some useful attributes like a name, role, background, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Persona(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "    background: str = \"\"\n",
    "    personality: str = \"\"\n",
    "    circumstances: str = \"\"\n",
    "    rules: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create/instantiate any `Persona` we want for our characters. Let's create our Bob and Alice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        background=\"Computer Science PhD.\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print `bob` we will see that it is automatically converted to natural language description, which is usefull when we want to create the actual prompt for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name: Bob\n",
      "Your role: great dad\n",
      "Your circumstances: Your daughter will talk to you\n",
      "Your background: Computer Science PhD.\n",
      "Your personality: an extremely happy person that likes to help people\n"
     ]
    }
   ],
   "source": [
    "print(bob_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we are working with a really complex persona and this default description is not good for your needs, you can overwrite it by defining your own `description()` method as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your awesome name is Bob and your awesome role is being a great dad\n"
     ]
    }
   ],
   "source": [
    "class PersonaCustom(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "\n",
    "    def description(self):\n",
    "        return f\"Your awesome name is {self.name} and your awesome role is being a {self.role}\"\n",
    "\n",
    "awesome_bob = PersonaCustom(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\"\n",
    ")\n",
    "\n",
    "# Let's print \"awesome_bob\" persona\n",
    "print(awesome_bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now know how to create personas, let's move to the fun part which is actually creating the actual generator.\n",
    "\n",
    "Fortunatelly, we can simply use `sdialog`'s built-in `PersonaDialogGenerator` class to generate our persona-based dialogues as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m1665669424\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHello!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mHow's everything going, sweetie?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm good dad! My birthday is coming up soon and I have a special theme in mind.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOh really? What kind of theme are you thinking about for your party?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI want it to be themed as Lord of The Rings. Can we do that?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat sounds like a fantastic idea! I'm sure we can make it happen.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThanks Dad! What kind of decorations do you think we could use?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mWe could have banners with Elvish script, some maps of Middle-earth, and even some Gandalf and Aragorn posters. How does that sound?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat sounds amazing! Can we also have a Hobbit cake?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mOf course, we can definitely get a Hobbit-themed cake or maybe one shaped like the One Ring!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mPerfect Dad, you always know how to make my day!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mThat's what dads are for. I'm glad you're excited! Let’s make this the best birthday ever.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThanks so much, dad! You’re the best!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[37mI love you too, my dear. Can't wait to see your smiling face on your special day.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sdialog.generators import PersonaDialogGenerator\n",
    "\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    persona_a=bob_persona,\n",
    "    persona_b=alice_persona,\n",
    ")\n",
    "\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Dialogue Generation for STAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ Before we begin this section, make sure you have the STAR dataset downloaded in your system, inside the `datasets` folder:\n",
    "> ```bash\n",
    "> cd datasets\n",
    "> git clone git@github.com:RasaHQ/STAR.git\n",
    "> ```\n",
    "> Make sure you have a `datasets/STAR` folder the `dialogues` and `tasks` folders inside.\n",
    "\n",
    "The [STAR](https://arxiv.org/pdf/2010.11853) dataset contains 6652 human-generated dialogues as JSON objects where files are named as `NUMBER.json`.\n",
    "\n",
    "Humans had to follow a well-defined set of instruction to generate the dialogue role playing the system (wizard) and the client (user).\n",
    "\n",
    "For instance, clicking [here](datasets/STAR/dialogues/1.json) we can open the file [`1.json`](datasets/STAR/dialogues/1.json) containing the first dialogue. For now, let's focus only on the `\"Scenario\"` field.\n",
    "\n",
    "For instance, for the dialogue in `1.json` it is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Domains\": [  # List of domains\n",
    "        \"doctor\"\n",
    "    ],\n",
    "    \"Happy\": true,  # Wheather or not the dialogue follos a happy path\n",
    "    \"MultiTask\": false,  # Wheather or not this dialogue involves more than one task\n",
    "    \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
    "    \"WizardTask\": \"Inform the user of his/her doctor's orders.\",\n",
    "    \"WizardCapabilities\": [  # List of flowcharts describing the each task the Wizard is cable of doing\n",
    "        {\n",
    "        \"Domain\": \"doctor\",\n",
    "        \"SchemaImage\": \"doctor_followup.jpg\",\n",
    "        \"Task\": \"doctor_followup\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We can use the `STAR` module from `sdialog` to read scenarios object from any dialogue in STAR given it's id given a STAR conversation id as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['doctor'],\n",
       " 'Happy': True,\n",
       " 'MultiTask': False,\n",
       " 'UserTask': 'You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.',\n",
       " 'WizardCapabilities': [{'Domain': 'doctor',\n",
       "   'SchemaImage': 'doctor_followup.jpg',\n",
       "   'Task': 'doctor_followup'}],\n",
       " 'WizardTask': \"Inform the user of his/her doctor's orders.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog.datasets import STAR\n",
    "\n",
    "# Let's first indicate where the dataset is located\n",
    "STAR.set_path(\"datasets/STAR/\")\n",
    "\n",
    "# Let's set the first dialogue as the target example\n",
    "TARGET_DIALOG = 1\n",
    "\n",
    "# Let's load the scenario of the first dialog\n",
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which corresponds to the following dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mHello, I'm really worried. I forgot what I'm supposed to do and forgot to write it down... What do I do?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mMy name is Alexis and my last doctor was Dr. Morgan, but now my doctor is Dr. Johnson and I forgot how to take my medicine.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYour instructions are: Take your medicine before you go to sleep. If you experience nausea, please contact your doctor immediately..\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mAre you sure I'm supposed to take it before bed? I don't go to sleep every day because my sleep schedule is totally off right now because of the Coronavirus.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYes. It must be before bed or it will not be effective.\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mOkay thank you. I will get back in touch if this doesn't help.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "original_dialogue = STAR.get_dialog(1)\n",
    "original_dialogue.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `scenario`, we can see that in this conversation, the user's behavior is defined by instructions given in natural language (`\"UserTask\"`), however, the system/wizard behavior is more rigidly defined as a graph describing the dialogue policy to followed (since system was expected to be more deterministic). These graphs are described as JSON objects storing the graph edges as key:value pairs (source:destination). We can find these graphs in the [`STAR/tasks`](datasets/STAR/tasks) folder.\n",
    "\n",
    "Ideally, we would like our `DialogGenerator` to generate dialogues for each different scenario. That is, given a `scenario` we would like generate multiple dialogues for it.\n",
    "\n",
    "To achieve this, we only need to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Fortunately, we can use the built-in `get_scenario_description()` method to do this, which takes an `scenario` as input and returns its natural language description containing all the details (including the system behavior described by the graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation is between a User and a AI assistant in the following domains: doctor.\n",
      "\n",
      "The User instructions are: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n",
      "The AI assistant instructions are: Inform the user of his/her doctor's orders.\n",
      "\n",
      "In addition, the AI assistant is instructed to follow specific flowcharts to address the tasks. Flowcharts are defined as graph described using DOT.\n",
      "The actual DOT for the current tasks are:\n",
      "\n",
      "The graph for the task 'doctor_followup' with domain 'doctor' is:\n",
      "```dot\n",
      "digraph doctor_followup  {\n",
      "    hello -> ask_name;\n",
      "    ask_name -> doctor_ask_doctor_name;\n",
      "    doctor_ask_doctor_name -> query;\n",
      "    query -> doctor_inform_doctors_instructions;\n",
      "    doctor_inform_doctors_instructions -> anything_else\n",
      "}\n",
      "```\n",
      "and one example responses for each node is provided in the following json:\n",
      "```json\n",
      "{\n",
      "  \"hello\": \"Hello, how can I help?\",\n",
      "  \"ask_name\": \"Could I get your name, please?\",\n",
      "  \"doctor_ask_doctor_name\": \"Who is your doctor?\",\n",
      "  \"doctor_inform_doctors_instructions\": \"Your instructions are: INSTRUCTIONS.\",\n",
      "  \"doctor_bye\": \"Thank you and goodbye.\",\n",
      "  \"anything_else\": \"Is there anything else that I can do for you?\"\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Finally, the following should be considered regarding the conversation:\n",
      "   1. The conversation follows the 'happy path', meaning the conversations goes according to what it is described in the flowcharts.\n",
      "   2. The user is calling to perform only the defined task (doctor_followup), nothing else.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(STAR.get_scenario_description(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the original graph in JSON describing the system's behavior have been converted to a [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) description which should be easier to interpret by the LLM, since DOT is a well-known format to describe graphs in plain text. \n",
    "\n",
    "Let's now put these two methods together and create a function that given a STAR dialogue ID will generate a natural language description of the scenario associated to it, simply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_description(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Then return it along its description in natural language\n",
    "    return scenario, STAR.get_scenario_description(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function now we have everything we need to generate synthethic dialogues for STAR that follows the same scenario as a given target real dialogue.\n",
    "\n",
    "For instance, let's say we want to generate dialogues following the same scenario as the first STAR dialogue, we can simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the scenario and description of the first dialogue\n",
    "scenario, description = get_dialog_scenario_description(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = DialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate multiple conversation that follows the same scenario as dialogue 1 of STAR dataset.\n",
    "> **Note**\n",
    "> Run the cell multiple times to get different conversations for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3277458858\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35m{\n",
      "  \"Domains\": [\n",
      "    \"doctor\"\n",
      "  ],\n",
      "  \"Happy\": true,\n",
      "  \"MultiTask\": false,\n",
      "  \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
      "  \"WizardCapabilities\": [\n",
      "    {\n",
      "      \"Domain\": \"doctor\",\n",
      "      \"SchemaImage\": \"doctor_followup.jpg\",\n",
      "      \"Task\": \"doctor_followup\"\n",
      "    }\n",
      "  ],\n",
      "  \"WizardTask\": \"Inform the user of his/her doctor's orders.\"\n",
      "}\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mHi there, could you please remind me of what Dr. Morgan told me about my medication?\u001b[0m\n",
      "\u001b[31m[AI] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mMy name is Alexis.\u001b[0m\n",
      "\u001b[31m[AI] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mDr. Morgan.\u001b[0m\n",
      "\u001b[31m[AI] \u001b[0mYour instructions are: Take your medication three times a day, 30 minutes before each meal.\u001b[0m\n",
      "\u001b[94m[User] \u001b[37mThank you very much!\u001b[0m\n",
      "\u001b[31m[AI] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the LLM is able to follow the scenario surprisegnly well, specially for the system part which is guided by a graph with pre-defined responses.\n",
    "\n",
    "Now update the generator to match a more challenging scenario, let's say that of dialogue 5100 that is multi-task and does not follow a happy path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['plane', 'weather'],\n",
       " 'Happy': False,\n",
       " 'MultiTask': True,\n",
       " 'UserTask': 'Come up with your own scenario!\\n\\nAbout you:\\n- Your name: Ben\\n\\n The AI Assistant can handle:\\n- Search for a flight (e.g. from Chicago to Pittsburgh)\\n- Book a flight (e.g. with id 193)\\n- Checking the weather forecast in different Cities (e.g. Chicago or Pittsburgh)',\n",
       " 'WizardCapabilities': [{'Domain': 'plane',\n",
       "   'SchemaImage': 'plane_search.jpg',\n",
       "   'Task': 'plane_search'},\n",
       "  {'Domain': 'plane', 'SchemaImage': 'plane_book.jpg', 'Task': 'plane_book'},\n",
       "  {'Domain': 'weather', 'SchemaImage': 'weather.jpg', 'Task': 'weather'}],\n",
       " 'WizardTask': 'Follow the flow charts and help the user.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario, description = get_dialog_scenario_description(5100)\n",
    "\n",
    "dialog_generator.set(description, scenario)\n",
    "\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate dialogues for it:\n",
    "\n",
    "_(run multi-times the call to generate different ones)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3526676935\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mHi there. First of all, could you book a flight for me with the ID 193 please?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mSure, my name is Ben.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mCan I have your flight ID, please?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mThe flight ID is 193. Wait a moment though, can you also check the weather in Pittsburgh for me before we proceed with booking?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mOf course, I'll do that after checking if your flight is available. Let's first see about the availability of the flight.\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37m\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mThe flight is available. Should I reserve it for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mI think there might be a problem, let's check the weather in Pittsburgh first and if everything looks good we'll book.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mSure thing, Ben. For what day would you like to know the weather forecast?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mLet's look at tomorrow's weather forecast for Pittsburgh.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mIt will be partly cloudy all day on tomorrow in Pittsburgh, with temperatures of around 18 degrees celsius.\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mOkay, that sounds good. Now let's book the flight with ID 193 please.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mThe flight is available. Should I reserve it for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mYes, go ahead and book it for me.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mRight, your flight is now reserved!\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37m\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[37mThat's all, thank you very much.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mYou're welcome! Thank you and goodbye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, in previous section we only had to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Likewise, now we have to find a way to create the right system and user personas for each scenario which means we have to return the right system and user `Persona`s for a given `scenario`.\n",
    "\n",
    "Fortunately, we can use the built-in `STAR.get_user_persona_for_scenario(scenario)` and `STAR.get_system_persona_for_scenario(scenario)` methods to achieve this.\n",
    "\n",
    "For instance, let's get the user persona for the `scenario` of the first dialogue above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your role: user calling a AI assistant that can perform multiple tasks in the following domains: doctor.\n",
      "\n",
      "The following should be considered regarding the conversation:\n",
      "   1. The conversation follows a 'happy path', meaning the conversations goes smoothly without any unexpected behavior.\n",
      "   2. The conversation involves only one task you were instructed to (doctor_followup), nothing else\n",
      "Your circumstances: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n"
     ]
    }
   ],
   "source": [
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "\n",
    "user_persona = STAR.get_user_persona_for_scenario(scenario)\n",
    "print(user_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to what we did in the previous subsection, we just need to define a function that given a dialogue ID can return its scenario as well as the system and user persona for it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_and_personas(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Get the personas\n",
    "    system = STAR.get_system_persona_for_scenario(scenario)\n",
    "    user = STAR.get_user_persona_for_scenario(scenario)\n",
    "    return scenario, system, user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, not we can simply create a `PersonaDialogGenerator` using the system and user personas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m1388934746\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mHello.\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[37mHi there, could you please help me remember what my doctor told me after my appointment last week?\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mOf course. Could I get your name, please?\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[37mMy name is Alexis.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[37mIt's Dr. Morgan.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mYour instructions are: Take the prescribed medication three times a day, before meals.\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[37mThank you so much! Is there anything else that I can do for you?\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mNo, thank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's get the personas and the scenario\n",
    "scenario, system, user = get_dialog_scenario_and_personas(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    persona_a=system,\n",
    "    persona_b=user,\n",
    "    scenario=scenario\n",
    ")\n",
    "\n",
    "# let's generate the dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for this tutorial, congrats! 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate one synthetic dialog for each happy `\"doctor_followup\"` dialog in STAR and save it to disk for later use.\n",
    "\n",
    "Let's first get all happy dialogues for this task using `sdialog`'s built-in `STAR.get_dialogs()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a0fbf57b224e7db62d4df48515a09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dialogs:   0%|          | 0/6652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of happy \"doctor_followup\" dialogues in STAR: 105\n"
     ]
    }
   ],
   "source": [
    "original_dialogs = STAR.get_dialogs(task_name=\"doctor_followup\", happy=True, multitask=False)\n",
    "print('Total number of happy \"doctor_followup\" dialogues in STAR:', len(original_dialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dialogues and save them in the path pointed by the `PATH_OUTPUT` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2422d61ed57d431aa9f0447be7c2ae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dialog generation:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH_OUTPUT = \"output/STAR/full-generation\"\n",
    "\n",
    "path_txt = os.path.join(PATH_OUTPUT, \"txt\")\n",
    "path_json = os.path.join(PATH_OUTPUT, \"json\")\n",
    "os.makedirs(path_txt, exist_ok=True)\n",
    "os.makedirs(path_json, exist_ok=True)\n",
    "\n",
    "for dialog in tqdm(original_dialogs, desc=\"Dialog generation\"):\n",
    "    if os.path.exists(os.path.join(path_txt, f\"{dialog.dialogId}.txt\")):\n",
    "        continue\n",
    "\n",
    "    scenario, description = STAR.get_dialog_scenario_description(dialog.dialogId)\n",
    "    dialog_generator = DialogGenerator(\n",
    "        model=MODEL_NAME,\n",
    "        dialogue_details=description,\n",
    "        scenario=scenario\n",
    "    )\n",
    "    dialog = dialog_generator.generate(id=dialog.dialogId, seed=dialog.dialogId)\n",
    "\n",
    "    # Normalize speaker names in each turn (since their also generated by the LLM)\n",
    "    for turn in dialog.turns:\n",
    "        turn.speaker = \"System\" if \"AI\" in turn.speaker else \"User\"\n",
    "\n",
    "    dialog.to_file(os.path.join(path_json, f\"{dialog.dialogId}.json\"))\n",
    "    dialog.to_file(os.path.join(path_txt, f\"{dialog.dialogId}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the files were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mjson\u001b[0m/\n",
      "\u001b[01;34mtxt\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls output/STAR/full-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "1848.txt\n",
      "1886.txt\n",
      "1896.txt\n",
      "1899.txt\n",
      "1942.txt\n",
      "1963.txt\n",
      "1970.txt\n",
      "2103.txt\n",
      "2228.txt\n",
      "2273.txt\n",
      "2487.txt\n",
      "2526.txt\n",
      "2578.txt\n",
      "2579.txt\n",
      "2624.txt\n",
      "2699.txt\n",
      "2733.txt\n",
      "3007.txt\n",
      "3024.txt\n",
      "3050.txt\n",
      "3071.txt\n",
      "3073.txt\n",
      "3086.txt\n",
      "3088.txt\n",
      "3110.txt\n",
      "3116.txt\n",
      "3126.txt\n",
      "3136.txt\n",
      "3155.txt\n",
      "3198.txt\n",
      "3202.txt\n",
      "3210.txt\n",
      "3234.txt\n",
      "3254.txt\n",
      "3264.txt\n",
      "3269.txt\n",
      "3274.txt\n",
      "3298.txt\n",
      "3316.txt\n",
      "3323.txt\n",
      "3329.txt\n",
      "3330.txt\n",
      "3356.txt\n",
      "3371.txt\n",
      "3391.txt\n",
      "3403.txt\n",
      "3418.txt\n",
      "3422.txt\n",
      "3437.txt\n",
      "3446.txt\n",
      "3454.txt\n",
      "3469.txt\n",
      "3494.txt\n",
      "3516.txt\n",
      "3528.txt\n",
      "3550.txt\n",
      "3652.txt\n",
      "3675.txt\n",
      "3743.txt\n",
      "3769.txt\n",
      "4055.txt\n",
      "4058.txt\n",
      "4067.txt\n",
      "4076.txt\n",
      "4082.txt\n",
      "4093.txt\n",
      "4100.txt\n",
      "4111.txt\n",
      "4159.txt\n",
      "4173.txt\n",
      "4191.txt\n",
      "4205.txt\n",
      "4214.txt\n",
      "4224.txt\n",
      "4231.txt\n",
      "4233.txt\n",
      "4253.txt\n",
      "4255.txt\n",
      "4263.txt\n",
      "4341.txt\n",
      "4349.txt\n",
      "4358.txt\n",
      "4381.txt\n",
      "4395.txt\n",
      "4403.txt\n",
      "4416.txt\n",
      "4459.txt\n",
      "4468.txt\n",
      "4477.txt\n",
      "4502.txt\n",
      "4515.txt\n",
      "4524.txt\n",
      "4536.txt\n",
      "4570.txt\n",
      "4591.txt\n",
      "4623.txt\n",
      "4653.txt\n",
      "4737.txt\n",
      "4743.txt\n",
      "4752.txt\n",
      "4851.txt\n",
      "4870.txt\n",
      "4875.txt\n",
      "9.txt\n"
     ]
    }
   ],
   "source": [
    "%ls output/STAR/full-generation/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Doctor-Patient Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now you have to generate synthetic doctor-patient conversations, is it better to do it using the \"description\" or the \"role-playing\" approach?\n",
    "\n",
    "- How would you define the Patient and Doctor personas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do your magic!\n",
    "class DoctorPersona(BasePersona):\n",
    "    pass\n",
    "\n",
    "class PatientPersona(BasePersona):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you initialize them? (e.g. concrete values for each defined attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor = DoctorPersona(\n",
    "    # TODO: assign values to the attribues!\n",
    ")\n",
    "patient = PatientPersona(\n",
    "    # TODO: assign values to the attribues!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our Personas and created our two doctor and patient personas, wwe can simply create a generator for them as we did before in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le'ts now create a dialogue generator for our doctor and patients\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    model=MODEL_NAME,\n",
    "    persona_a=doctor,\n",
    "    persona_b=patient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate as many dialogues for them: _(running the cell multiple times)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate the doctor-patient dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you think of an `scenario` object for doctor-patient conversations? What would this scenario contain?\n",
    "\n",
    "> 💡 **Hint:** what is it that you want to keep track/control when generating the conversations? (different dissises? different outcomes? different skills? etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = {}  # TODO: define your own, perhaps better to use pydantic's BaseModel instead of a dict {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had such `scenario` then we could define a function that could return the right doctor and patient for the provided scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doctor_patient_for_scenario(scenario):\n",
    "    # TODO: do some magic to return the right doctor\n",
    "    #       and patient personas for the given scenario\n",
    "    # doctor = DoctorPersona()\n",
    "    # patient = DoctorPersona()\n",
    "    return doctor, patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we could finally use to create generators for different `scenarios`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_for_scenario(scenario):\n",
    "    # Get the right doctor and patient personas for the given scenario\n",
    "    doctor, patient = get_doctor_patient_for_scenario(scenario)\n",
    "\n",
    "    # Create and return a dialogue generator for them\n",
    "    return PersonaDialogGenerator(\n",
    "        model=MODEL_NAME,\n",
    "        persona_a=doctor,\n",
    "        persona_b=patient,\n",
    "        scenario=scenario,\n",
    "        # dialogue_details=\"\"  # TODO: optional, in case scenario also requires defining certain properties outside the personas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator_for_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will allow us to generate multiple dialogues belonging to the same `scenario`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, huh? Congrats for finalizing the tutorial! you did a great job! 😎"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
